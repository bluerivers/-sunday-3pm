# 7/16 ML 스터디 시즌 2 8주 차

## 진행사항
- poker NN
 - learning rate를 0.01 이하로 하여, step 2000 번 정도 돌리면 유의미하게 90% 이상의 결과를 낼 수 있음
 - 97%까지 준비해왔다가 다같이 설정값을 조정해가면서 99%까지 안정적으로 결과를 냄
 - dropout도 적절하게 적용해야 좋은 결과를 냄. 오히려 역효과를 낼 수도 있음

## 차주 진행사항
- N주간 방학
 
## 후기

### Luca
* learning rate를 줄여서 진행할 생각을 못해봤다. 계속 0.1로만 했던 이유가 CPU로 돌릴려니 500 step 돌리는데만 해도 거의 5분 걸렸기 때문. 0.01 정도로는 1000은 돌려야 증가하는게 보이는데 스터디에서 빠르게 해볼 생각만 했다. 눈에 보이는 cost graph는 이미 수렴했던 것으로 보였다.
* 결국, 빠르게 가설을 검증할 수 있는 환경이 필요하다. 만건 이상의 데이터와 85개의 많은 feature는 GPU로 돌리는게 필수적임을 깨달았다. 
* dropout을 input에서 제거하면 더욱 좋은 결과를 보였다. 또한 마지막 hidden layer에서도 제거하니 더욱 좋은 결과를 보였다. 처음에는 dropout을 전부 제거했을 때, 96% 정도가 나오니, dropout은 유해하다!라고 단순하게 생각해버렸지만, jay의 도움으로 0.95 정도로 hidden layer에 dropout을 적용하였더니 99%까지 안정적으로 결과가 나왔다.
* Adam, AdaDelta, Gradient optimizer를 차례로 적용해보았는데, Adam 이외에는 50% 정도로 현저하게 낮은 결과가 나왔다. Adam이 feature마다 learning rate를 달리 주니 유연하고 좋은 결과를 주는 듯하다.
* 설정값을 달리하며 학습을 시킬 때, 각 설정과 training 결과를 저장해두는 것이 설정값들의 영향력을 되돌아 볼 수 있게 하였다. 대충 찍어서 값을 넣으며 학습 결과를 얻으면 내가 얻는 것도 찍는 거 이상 얻을 수 없다.
* 이 정도 정확도로 올렸으면 잘 학습되었다고 생각해도 계속 올라갈 여지가 있었다. 여기가 한계임을 어떻게 짚어내야할까?

### Gin
* 복잡하게 NN을 구성하기 보다는 hidden layer / dropout rate / learning rate 등을 조절해가면서 차례로 결과의 변화를 보면서 진행해나가야 함
* 마구 돌리기 보다는 데이터 구성 / NN 구성 등에 대해 가설/검증/피드백 단계를 거쳐야 한다고 생각했음


### Brad
* Optimizer선택은 최적이라고 알려진걸 쓰는게 가장 좋은 것 같다
* 한방에 효과적인걸 뽑아내기보다는, 하나 하나 바꿔가면서 최적해를 찾아가야 한다는걸 느끼면서 장비가 중요하다는 생각이 들었다
* 너무 내부에 blind된 부분이 많아서 잘 알고 하려면 공부가 많이 필요한 듯 하다

### Jay
* luca의 실험에 의하면 learning rate과 optimizer가 생각보다 큰 영향을 줌
* batch normalization에 대해 보는 중인데 아직 인트로밖에 안읽어서 자세하게는 모르겠지만
일단 mini batch를 쓰는거는 stochastic gradient descent와 batch gradient descent의 중간을 쓰는 느낌인듯.
BGD는 트레이닝 전체에 대해 계산해서 weight 조정하니까 안정적인데 느리게 수렴함.
SGD는 각 example에 대해 조정하니 빠르게 수렴하지만
optimum에 도달하지 못하고 근처에서 진동하는 경우가 많음 (outlier 영향도 많이 받을테고).
그 중간정도가 mini batch 쓰는 건데 핵좋음
