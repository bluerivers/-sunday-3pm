# 7/16 ML 스터디 시즌 2 8주 차

## 진행사항
- poker NN
 - learning rate를 0.01 이하로 하여, step 2000 번 정도 돌리면 유의미하게 90% 이상의 결과를 낼 수 있음
 - 97%까지 준비해왔다가 다같이 설정값을 조정해가면서 99%까지 안정적으로 결과를 냄
 - dropout도 적절하게 적용해야 좋은 결과를 냄. 오히려 역효과를 낼 수도 있음

## 차주 진행사항
- N주간 방학
 
## 후기

### Luca
* learning rate를 줄여서 진행할 생각을 못해봤다. 계속 0.1로만 했던 이유가 CPU로 돌릴려니 500 step 돌리는데만 해도 거의 5분 걸렸기 때문. 0.01 정도로는 1000은 돌려야 증가하는게 보이는데 스터디에서 빠르게 해볼 생각만 했다. 눈에 보이는 cost graph는 이미 수렴했던 것으로 보였다.
* 결국, 빠르게 가설을 검증할 수 있는 환경이 필요하다. 만건 이상의 데이터와 85개의 많은 feature는 GPU로 돌리는게 필수적임을 깨달았다. 
* dropout을 input에서 제거하면 더욱 좋은 결과를 보였다. 또한 마지막 hidden layer에서도 제거하니 더욱 좋은 결과를 보였다. 처음에는 dropout을 전부 제거했을 때, 96% 정도가 나오니, dropout은 유해하다!라고 단순하게 생각해버렸지만, jay의 도움으로 0.95 정도로 hidden layer에 dropout을 적용하였더니 99%까지 안정적으로 결과가 나왔다.
* Adam, AdaDelta, Gradient optimizer를 차례로 적용해보았는데, Adam 이외에는 50% 정도로 현저하게 낮은 결과가 나왔다. Adam이 feature마다 learning rate를 달리 주니 유연하고 좋은 결과를 주는 듯하다.
* 설정값을 달리하며 학습을 시킬 때, 각 설정과 training 결과를 저장해두는 것이 설정값들의 영향력을 되돌아 볼 수 있게 하였다. 대충 찍어서 값을 넣으며 학습 결과를 얻으면 내가 얻는 것도 찍는 거 이상 얻을 수 없다.
* 이 정도 정확도로 올렸으면 잘 학습되었다고 생각해도 계속 올라갈 여지가 있었다. 여기가 한계임을 어떻게 짚어내야할까?

### Gin

### Brad

### Jay
