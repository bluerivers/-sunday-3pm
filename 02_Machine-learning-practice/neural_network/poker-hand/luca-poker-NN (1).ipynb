{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import genfromtxt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is z-score that value minus mean divided by standard deviation\n",
    "# http://duramecho.com/Misc/WhyMinusOneInSd.html\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis=0)\n",
    "    sigma = np.std(dataset,axis=0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "def append_bias_reshape(features,labels):\n",
    "    n_training_samples = features.shape[0]\n",
    "    n_dim = features.shape[1]\n",
    "    # 아래꺼 np.c_만 하면 되는거 아닌가? reshape는 왜하지\n",
    "    f = np.reshape(np.c_[np.ones(n_training_samples),features],[n_training_samples,n_dim + 1])\n",
    "    l = np.reshape(labels,[n_training_samples,1])\n",
    "    return f, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    df = pd.read_csv(file_name, sep=',', header=None)\n",
    "    return df\n",
    "\n",
    "def merge_column(df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in range(df.shape[1]/2):\n",
    "        new_df[i] = df[i] * df[i+1]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0   1   2   3   4   5   6   7   8   9   10\n",
      "25005   3   9   2   6   4  11   4  12   2   4   0\n",
      "25006   4   1   4  10   3  13   3   4   1  10   1\n",
      "25007   2   1   2  10   4   4   4   1   4  13   1\n",
      "25008   2  12   4   3   1  10   1  12   4   9   1\n",
      "25009   1   7   3  11   3   3   4   8   3   7   1\n",
      "        0   1   2   3   4   5   6   7   8   9   10\n",
      "999995   3   1   1  12   2   9   4   9   2   6   1\n",
      "999996   3   3   4   5   2   7   1   4   4   3   1\n",
      "999997   1  11   4   7   3   9   1  13   2   7   1\n",
      "999998   3  11   1   8   1   1   3  13   2   8   1\n",
      "999999   2   5   2   9   4   9   2   3   3   3   2\n"
     ]
    }
   ],
   "source": [
    "df = read_data('poker-hand-training-true.data')\n",
    "df_test = read_data('poker-hand-testing.data')\n",
    "print(df.tail())\n",
    "print(df_test.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.24653454  14.06103538   1.25836179  14.19009001   1.26146106\n",
      "  14.02483064   1.24547499  14.04111337   1.25156226  13.99941097]\n",
      "(25010, 10) (25010, 1)\n"
     ]
    }
   ],
   "source": [
    "features = df.iloc[:, :-1].values\n",
    "labels = df.iloc[:, -1:].values\n",
    "print(stats.describe(features).variance)\n",
    "print(features.shape, labels.shape)\n",
    "\n",
    "features_test = df_test.iloc[:, :-1].values\n",
    "labels_test = df_test.iloc[:, -1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00003999  1.00003999  1.00003999  1.00003999  1.00003999  1.00003999\n",
      "  1.00003999  1.00003999  1.00003999  1.00003999]\n",
      "[ 1.000001  1.000001  1.000001  1.000001  1.000001  1.000001  1.000001\n",
      "  1.000001  1.000001  1.000001]\n"
     ]
    }
   ],
   "source": [
    "features = feature_normalize(features)\n",
    "print(stats.describe(features).variance)\n",
    "\n",
    "features_test = feature_normalize(features_test)\n",
    "print(stats.describe(features_test).variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1\n"
     ]
    }
   ],
   "source": [
    "train_x = features\n",
    "train_y = labels\n",
    "test_x = features_test\n",
    "test_y = labels_test\n",
    "\n",
    "feature_count = train_x.shape[1]\n",
    "label_count = train_y.shape[1]\n",
    "print(feature_count, label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot Tensor(\"one_hot_20:0\", shape=(?, 1, 10), dtype=float32)\n",
      "reshape Tensor(\"Reshape_80:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 50000\n",
    "learning_rate = 0.1\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "nb_classes = 10\n",
    "\n",
    "# x는 float32 로 할 필요가 있나? normalized 되었기때문에 float32 써야함\n",
    "X = tf.placeholder(tf.float32,[None,feature_count])\n",
    "Y = tf.placeholder(tf.int32,[None,label_count])\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot\", Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape\", Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.1))\n",
    "\n",
    "def model(X, p_keep_input, p_keep_hidden): # this network is the same as the previous one except with an extra hidden layer + dropout\n",
    "    s_1 = feature_count + 4\n",
    "    s_2 = feature_count + 4\n",
    "    s_3 = feature_count\n",
    "    s_4 = feature_count\n",
    "    s_5 = feature_count\n",
    "    \n",
    "    w_h = init_weights([feature_count, s_1])\n",
    "    w_h2 = init_weights([s_1, s_2])\n",
    "#     w_h3 = init_weights([s_2, s_3])\n",
    "#     w_h4 = init_weights([s_3, s_4])\n",
    "    w_o = init_weights([s_2, nb_classes])\n",
    "    \n",
    "    b = tf.Variable(tf.random_normal([s_1]))\n",
    "    b2 = tf.Variable(tf.random_normal([s_2]))\n",
    "#     b3 = tf.Variable(tf.random_normal([s_3]))\n",
    "    b_o = tf.Variable(tf.random_normal([nb_classes]))\n",
    "    \n",
    "    X = tf.nn.dropout(X, p_keep_input)\n",
    "    h = tf.nn.relu(tf.matmul(X, w_h) + b)\n",
    "    h = tf.nn.dropout(h, p_keep_hidden)\n",
    "    h2 = tf.nn.relu(tf.matmul(h, w_h2) + b2)\n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "#     h3 = tf.nn.relu(tf.matmul(h2, w_h3) + b3)\n",
    "#     h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    \n",
    "#     b4 = tf.Variable(tf.random_normal([s_4]))\n",
    "#     h4 = tf.nn.softmax(tf.matmul(h3, w_h4) + b4)\n",
    "    \n",
    "#     h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "#     b5 = tf.Variable(tf.random_normal([s_5]))\n",
    "#     w_h5 = init_weights([s_4, s_5])\n",
    "#     h5 = tf.nn.softmax(tf.matmul(h4, w_h5) + b5)\n",
    "    \n",
    "# #     h5 = tf.nn.dropout(h5, p_keep_hidden)\n",
    "#     b6 = tf.Variable(tf.random_normal([feature_count]))\n",
    "#     w_h6 = init_weights([feature_count, feature_count])\n",
    "#     h6 = tf.nn.relu(tf.matmul(h5, w_h6) + b6)\n",
    "    \n",
    "# #     h6 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "#     b7 = tf.Variable(tf.random_normal([feature_count]))\n",
    "#     w_h7 = init_weights([feature_count, feature_count])\n",
    "#     h7 = tf.nn.relu(tf.matmul(h6, w_h7) + b7)\n",
    "    \n",
    "# #     h7 = tf.nn.dropout(h7, p_keep_hidden)\n",
    "#     b8 = tf.Variable(tf.random_normal([feature_count]))\n",
    "#     w_h8 = init_weights([feature_count, feature_count])\n",
    "#     h8 = tf.nn.relu(tf.matmul(h7, w_h8) + b8)\n",
    "    \n",
    "# #     h8 = tf.nn.dropout(h8, p_keep_hidden)\n",
    "#     b9 = tf.Variable(tf.random_normal([feature_count]))\n",
    "#     w_h9 = init_weights([feature_count, feature_count])\n",
    "#     h9 = tf.nn.relu(tf.matmul(h8, w_h9) + b9)\n",
    "    \n",
    "# #     h9 = tf.nn.dropout(h9, p_keep_hidden)\n",
    "#     b10 = tf.Variable(tf.random_normal([feature_count]))\n",
    "#     w_h10 = init_weights([feature_count, feature_count])\n",
    "#     h10 = tf.nn.relu(tf.matmul(h9, w_h10) + b10)\n",
    "    \n",
    "#     h10 = tf.nn.dropout(h10, p_keep_hidden)\n",
    "\n",
    "    \n",
    "    \n",
    "    return tf.matmul(h2, w_o) + b_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w0, b0, h0 = get_class_logits()\n",
    "\n",
    "p_keep_input = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "h0 = model(X, p_keep_input, p_keep_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/carpedm20/DCGAN-tensorflow/issues/99\n",
    "# all_logits = tf.concat([h0, h1, h2], 1)\n",
    "\n",
    "# regularizers = tf.nn.l2_loss(w0)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=h0, labels=Y_one_hot))\n",
    "# + 1e-4*regularizers\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.argmax(h0, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25010, 10) (25010, 1)\n",
      "(1000000, 10) (1000000, 1)\n",
      "(?, 10) (?, 1)\n",
      "Step:     0\tLoss: 1.564\tAcc: 42.38%\n",
      "Step:  1000\tLoss: 0.872\tAcc: 58.71%\n",
      "Step:  2000\tLoss: 0.879\tAcc: 59.62%\n",
      "Step:  3000\tLoss: 0.901\tAcc: 58.41%\n",
      "Step:  4000\tLoss: 0.880\tAcc: 58.94%\n",
      "Step:  5000\tLoss: 0.877\tAcc: 58.98%\n",
      "Step:  6000\tLoss: 0.886\tAcc: 58.00%\n",
      "Step:  7000\tLoss: 0.871\tAcc: 60.89%\n",
      "Step:  8000\tLoss: 0.868\tAcc: 60.86%\n",
      "Step:  9000\tLoss: 0.870\tAcc: 60.34%\n",
      "Step: 10000\tLoss: 0.866\tAcc: 60.75%\n",
      "Step: 11000\tLoss: 0.872\tAcc: 60.22%\n",
      "Step: 12000\tLoss: 0.849\tAcc: 61.70%\n",
      "Step: 13000\tLoss: 0.892\tAcc: 59.33%\n",
      "Step: 14000\tLoss: 0.873\tAcc: 60.30%\n",
      "Step: 15000\tLoss: 0.891\tAcc: 57.28%\n",
      "Step: 16000\tLoss: 0.870\tAcc: 60.93%\n",
      "Step: 17000\tLoss: 0.864\tAcc: 60.28%\n",
      "Step: 18000\tLoss: 0.861\tAcc: 60.77%\n",
      "Step: 19000\tLoss: 0.863\tAcc: 61.22%\n",
      "Step: 20000\tLoss: 0.857\tAcc: 61.60%\n",
      "Step: 21000\tLoss: 0.865\tAcc: 60.54%\n",
      "Step: 22000\tLoss: 0.870\tAcc: 61.10%\n",
      "Step: 23000\tLoss: 0.880\tAcc: 60.99%\n",
      "Step: 24000\tLoss: 0.860\tAcc: 61.05%\n",
      "Step: 25000\tLoss: 0.863\tAcc: 61.30%\n",
      "Step: 26000\tLoss: 0.868\tAcc: 59.65%\n",
      "Step: 27000\tLoss: 0.860\tAcc: 61.36%\n",
      "Step: 28000\tLoss: 0.858\tAcc: 61.52%\n",
      "Step: 29000\tLoss: 0.861\tAcc: 60.36%\n",
      "Step: 30000\tLoss: 0.874\tAcc: 60.97%\n",
      "Step: 31000\tLoss: 0.883\tAcc: 58.18%\n",
      "Step: 32000\tLoss: 0.858\tAcc: 61.09%\n",
      "Step: 33000\tLoss: 0.868\tAcc: 59.43%\n",
      "Step: 34000\tLoss: 0.860\tAcc: 61.34%\n",
      "Step: 35000\tLoss: 0.857\tAcc: 61.55%\n",
      "Step: 36000\tLoss: 0.862\tAcc: 61.23%\n",
      "Step: 37000\tLoss: 0.896\tAcc: 60.58%\n",
      "Step: 38000\tLoss: 0.860\tAcc: 61.32%\n",
      "Step: 39000\tLoss: 0.857\tAcc: 61.52%\n",
      "Step: 40000\tLoss: 0.865\tAcc: 60.13%\n",
      "Step: 41000\tLoss: 0.859\tAcc: 61.24%\n",
      "Step: 42000\tLoss: 0.856\tAcc: 61.58%\n",
      "Step: 43000\tLoss: 0.861\tAcc: 61.03%\n",
      "Step: 44000\tLoss: 0.905\tAcc: 60.54%\n",
      "Step: 45000\tLoss: 0.864\tAcc: 61.09%\n",
      "Step: 46000\tLoss: 0.862\tAcc: 61.36%\n",
      "Step: 47000\tLoss: 0.864\tAcc: 61.35%\n",
      "Step: 48000\tLoss: 0.858\tAcc: 61.22%\n",
      "Step: 49000\tLoss: 0.862\tAcc: 60.30%\n",
      "Step: 50000\tLoss: 0.859\tAcc: 61.41%\n",
      "(1000000,)\n",
      "Test Accuracy: 0.611308\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)\n",
    "print(X.shape, Y.shape)\n",
    "training_dropout_i = 1.0\n",
    "training_dropout_h = 1.0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(training_epochs + 1):\n",
    "        sess.run(optimizer, feed_dict={X: train_x, Y: train_y, p_keep_input: training_dropout_i, p_keep_hidden: training_dropout_h})\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={\n",
    "                                 X: train_x, Y: train_y, p_keep_input: training_dropout_i, p_keep_hidden: training_dropout_h})\n",
    "        cost_history = np.append(cost_history, acc)\n",
    "        if step % 1000 == 0:\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(\n",
    "                step, loss, acc))\n",
    "            \n",
    "    # Test model and check accuracy\n",
    "    pre = tf.argmax(h0, 1)\n",
    "    test_yy = np.transpose(test_y.ravel())\n",
    "    print(test_yy.shape)\n",
    "    correct_prediction = tf.equal(pre, test_yy)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('Test Accuracy:', sess.run(accuracy, feed_dict={X: test_x, \n",
    "                                                         p_keep_input: 1.0,\n",
    "                                                         p_keep_hidden: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50002,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCgQREQEUCCIi4lEgiIgIoEAQERHP/weSkO5m03lCegAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20c17b116d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(cost_history.shape)\n",
    "plt.plot(range(len(cost_history)),cost_history)\n",
    "plt.axis([0,training_epochs,0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
