{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import genfromtxt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is z-score that value minus mean divided by standard deviation\n",
    "# http://duramecho.com/Misc/WhyMinusOneInSd.html\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis=0)\n",
    "    sigma = np.std(dataset,axis=0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "def append_bias_reshape(features,labels):\n",
    "    n_training_samples = features.shape[0]\n",
    "    n_dim = features.shape[1]\n",
    "    f = np.c_[np.ones(n_training_samples),features],[n_training_samples,n_dim + 1]\n",
    "    l = np.reshape(labels,[n_training_samples,1])\n",
    "    return f, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    df = pd.read_csv(file_name, sep=',', header=None)\n",
    "    return df\n",
    "\n",
    "def merge_column(df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in range(df.shape[1]/2):\n",
    "        new_df[i] = df[i] * df[i+1]\n",
    "    return new_df\n",
    "\n",
    "# https://stackoverflow.com/a/42523230\n",
    "def one_hot(df, cols):\n",
    "    \"\"\"\n",
    "    @param df pandas DataFrame\n",
    "    @param cols a list of columns to encode \n",
    "    @return a DataFrame with one-hot encoding\n",
    "    \"\"\"\n",
    "    for each in cols:\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        del df[each]\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       10  0_1  0_2  0_3  0_4  1_1  1_2  1_3  1_4  1_5  ...   9_4  9_5  9_6  \\\n",
      "25005   0    0    0    1    0    0    0    0    0    0  ...     1    0    0   \n",
      "25006   1    0    0    0    1    1    0    0    0    0  ...     0    0    0   \n",
      "25007   1    0    1    0    0    1    0    0    0    0  ...     0    0    0   \n",
      "25008   1    0    1    0    0    0    0    0    0    0  ...     0    0    0   \n",
      "25009   1    1    0    0    0    0    0    0    0    0  ...     0    0    0   \n",
      "\n",
      "       9_7  9_8  9_9  9_10  9_11  9_12  9_13  \n",
      "25005    0    0    0     0     0     0     0  \n",
      "25006    0    0    0     1     0     0     0  \n",
      "25007    0    0    0     0     0     0     1  \n",
      "25008    0    0    1     0     0     0     0  \n",
      "25009    1    0    0     0     0     0     0  \n",
      "\n",
      "[5 rows x 86 columns]\n",
      "        10  0_1  0_2  0_3  0_4  1_1  1_2  1_3  1_4  1_5  ...   9_4  9_5  9_6  \\\n",
      "999995   1    0    0    1    0    1    0    0    0    0  ...     0    0    1   \n",
      "999996   1    0    0    1    0    0    0    1    0    0  ...     0    0    0   \n",
      "999997   1    1    0    0    0    0    0    0    0    0  ...     0    0    0   \n",
      "999998   1    0    0    1    0    0    0    0    0    0  ...     0    0    0   \n",
      "999999   2    0    1    0    0    0    0    0    0    1  ...     0    0    0   \n",
      "\n",
      "        9_7  9_8  9_9  9_10  9_11  9_12  9_13  \n",
      "999995    0    0    0     0     0     0     0  \n",
      "999996    0    0    0     0     0     0     0  \n",
      "999997    1    0    0     0     0     0     0  \n",
      "999998    0    1    0     0     0     0     0  \n",
      "999999    0    0    0     0     0     0     0  \n",
      "\n",
      "[5 rows x 86 columns]\n"
     ]
    }
   ],
   "source": [
    "df = read_data('poker-hand-training-true.data')\n",
    "df_test = read_data('poker-hand-testing.data')\n",
    "\n",
    "df = one_hot(df, df.iloc[:,:-1].columns)\n",
    "df_test = one_hot(df_test, df_test.iloc[:,:-1].columns)\n",
    "print(df.tail())\n",
    "print(df_test.tail())\n",
    "# df[10].value_counts().sort_index().plot('bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.18544144  0.18841386  0.18745751  0.18869141  0.07297093  0.07158867\n",
      "  0.06959014  0.07084475  0.06992965  0.06884207  0.07226361  0.07182504\n",
      "  0.07246585  0.06942026  0.07108162  0.07155489  0.07070933  0.18863199\n",
      "  0.18733744  0.1855836   0.18845355  0.07307185  0.07091245  0.07060773\n",
      "  0.07037055  0.06833114  0.072095    0.07060773  0.07013321  0.06942026\n",
      "  0.07094629  0.07125072  0.0715211   0.07381113  0.18705685  0.18661465\n",
      "  0.18550239  0.19079198  0.07003145  0.07053998  0.07239845  0.07138594\n",
      "  0.0708786   0.06965806  0.07111545  0.07060773  0.06856966  0.07357607\n",
      "  0.0706416   0.07313913  0.07053998  0.18783694  0.1874175   0.18873101\n",
      "  0.18602939  0.07300457  0.07313913  0.07354248  0.07026885  0.0696241\n",
      "  0.0706416   0.07165622  0.06901222  0.07175752  0.07101396  0.07199379\n",
      "  0.06809246  0.06931829  0.18861217  0.18588772  0.18873101  0.18677563\n",
      "  0.07256692  0.07212873  0.07067547  0.07175752  0.07233104  0.06999752\n",
      "  0.06792188  0.07337447  0.07125072  0.07182504  0.07037055  0.06894417\n",
      "  0.06992965]\n",
      "(25010, 85) (25010, 1)\n"
     ]
    }
   ],
   "source": [
    "features = df.iloc[:, 1:].values\n",
    "labels = df.iloc[:, :1].values\n",
    "print(stats.describe(features).variance)\n",
    "print(features.shape, labels.shape)\n",
    "\n",
    "features_test = df_test.iloc[:, :-1].values\n",
    "labels_test = df_test.iloc[:, -1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 1\n"
     ]
    }
   ],
   "source": [
    "train_x = features\n",
    "train_y = labels\n",
    "test_x = features_test\n",
    "test_y = labels_test\n",
    "\n",
    "feature_count = train_x.shape[1]\n",
    "label_count = train_y.shape[1]\n",
    "print(feature_count, label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot Tensor(\"one_hot_20:0\", shape=(?, 1, 10), dtype=float32)\n",
      "reshape Tensor(\"Reshape_71:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 2000\n",
    "learning_rate = 0.1\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "nb_classes = 10\n",
    "\n",
    "# x는 float32 로 할 필요가 있나? normalized 되었기때문에 float32 써야함 or dropout에서 float를 씀\n",
    "X = tf.placeholder(tf.float32,[None,feature_count])\n",
    "Y = tf.placeholder(tf.int32,[None,label_count])\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot\", Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape\", Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.1)), tf.Variable(tf.random_normal([shape[1]]))\n",
    "\n",
    "def make_hidden_layer(previous_h, weight, bias, p_keep_hidden):\n",
    "    h = tf.nn.relu(tf.matmul(previous_h, weight) + bias)\n",
    "    h = tf.nn.dropout(h, p_keep_hidden)\n",
    "    return h\n",
    "\n",
    "def model(X, p_keep_input, p_keep_hidden): # this network is the same as the previous one except with an extra hidden layer + dropout\n",
    "    s_1 = feature_count + 2\n",
    "    s_2 = feature_count + 2\n",
    "    s_3 = feature_count\n",
    "    s_4 = feature_count\n",
    "    \n",
    "    w_h, b = init_weights([feature_count, s_1])\n",
    "    w_h2, b2 = init_weights([s_1, s_2])\n",
    "    w_h3, b3 = init_weights([s_2, s_3])\n",
    "    w_h4, b4 = init_weights([s_3, s_4])\n",
    "    w_o, b_o = init_weights([s_4, nb_classes])\n",
    "    \n",
    "    X = tf.nn.dropout(X, p_keep_input)\n",
    "    h = make_hidden_layer(X, w_h, b, p_keep_hidden)\n",
    "    h2 = make_hidden_layer(h, w_h2, b2, p_keep_hidden)\n",
    "    h3 = make_hidden_layer(h2, w_h3, b3, p_keep_hidden)\n",
    "    h4 = make_hidden_layer(h3, w_h4, b4, p_keep_hidden)\n",
    "    \n",
    "    return tf.matmul(h4, w_o) + b_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_keep_input = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "h0 = model(X, p_keep_input, p_keep_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=h0, labels=Y_one_hot))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.argmax(h0, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25010, 85) (25010, 1)\n",
      "(1000000, 85) (1000000, 1)\n",
      "(?, 85) (?, 1)\n",
      "Step:     0\tLoss: 54.624\tAcc: 49.00%\n",
      "Step:  1000\tLoss: 0.986\tAcc: 49.95%\n",
      "Step:  2000\tLoss: 0.985\tAcc: 49.95%\n",
      "(1000000,)\n",
      "Test Accuracy: 0.923413\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)\n",
    "print(X.shape, Y.shape)\n",
    "training_dropout_i = 0.8\n",
    "training_dropout_h = 0.7\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(training_epochs + 1):\n",
    "        sess.run(optimizer, feed_dict={X: train_x, Y: train_y, p_keep_input: training_dropout_i, p_keep_hidden: training_dropout_h})\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={\n",
    "                                 X: train_x, Y: train_y, p_keep_input: training_dropout_i, p_keep_hidden: training_dropout_h})\n",
    "        cost_history = np.append(cost_history, acc)\n",
    "        if step % 1000 == 0:\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(\n",
    "                step, loss, acc))\n",
    "            \n",
    "    # Test model and check accuracy\n",
    "    pre = tf.argmax(h0, 1)\n",
    "    test_yy = np.transpose(test_y.ravel())\n",
    "    print(test_yy.shape)\n",
    "    correct_prediction = tf.equal(pre, test_yy)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('Test Accuracy:', sess.run(accuracy, feed_dict={X: test_x, \n",
    "                                                         p_keep_input: 1.0,\n",
    "                                                         p_keep_hidden: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2002,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkkDc9ggAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19eff378be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(cost_history.shape)\n",
    "plt.plot(range(len(cost_history)),cost_history)\n",
    "plt.axis([0,training_epochs,0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
