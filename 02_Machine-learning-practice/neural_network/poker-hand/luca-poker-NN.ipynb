{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import genfromtxt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is z-score that value minus mean divided by standard deviation\n",
    "# http://duramecho.com/Misc/WhyMinusOneInSd.html\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis=0)\n",
    "    sigma = np.std(dataset,axis=0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "def append_bias_reshape(features,labels):\n",
    "    n_training_samples = features.shape[0]\n",
    "    n_dim = features.shape[1]\n",
    "    f = np.c_[np.ones(n_training_samples),features],[n_training_samples,n_dim + 1]\n",
    "    l = np.reshape(labels,[n_training_samples,1])\n",
    "    return f, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    df = pd.read_csv(file_name, sep=',', header=None)\n",
    "    return df\n",
    "\n",
    "def merge_column(df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in range(df.shape[1]/2):\n",
    "        new_df[i] = df[i] * df[i+1]\n",
    "    return new_df\n",
    "\n",
    "# https://stackoverflow.com/a/42523230\n",
    "def one_hot(df, cols):\n",
    "    \"\"\"\n",
    "    @param df pandas DataFrame\n",
    "    @param cols a list of columns to encode \n",
    "    @return a DataFrame with one-hot encoding\n",
    "    \"\"\"\n",
    "    for each in cols:\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        del df[each]\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       10  0_1  0_2  0_3  0_4  1_1  1_2  1_3  1_4  1_5  ...   9_4  9_5  9_6  \\\n",
      "25005   0    0    0    1    0    0    0    0    0    0  ...     1    0    0   \n",
      "25006   1    0    0    0    1    1    0    0    0    0  ...     0    0    0   \n",
      "25007   1    0    1    0    0    1    0    0    0    0  ...     0    0    0   \n",
      "25008   1    0    1    0    0    0    0    0    0    0  ...     0    0    0   \n",
      "25009   1    1    0    0    0    0    0    0    0    0  ...     0    0    0   \n",
      "\n",
      "       9_7  9_8  9_9  9_10  9_11  9_12  9_13  \n",
      "25005    0    0    0     0     0     0     0  \n",
      "25006    0    0    0     1     0     0     0  \n",
      "25007    0    0    0     0     0     0     1  \n",
      "25008    0    0    1     0     0     0     0  \n",
      "25009    1    0    0     0     0     0     0  \n",
      "\n",
      "[5 rows x 86 columns]\n",
      "        10  0_1  0_2  0_3  0_4  1_1  1_2  1_3  1_4  1_5  ...   9_4  9_5  9_6  \\\n",
      "999995   1    0    0    1    0    1    0    0    0    0  ...     0    0    1   \n",
      "999996   1    0    0    1    0    0    0    1    0    0  ...     0    0    0   \n",
      "999997   1    1    0    0    0    0    0    0    0    0  ...     0    0    0   \n",
      "999998   1    0    0    1    0    0    0    0    0    0  ...     0    0    0   \n",
      "999999   2    0    1    0    0    0    0    0    0    1  ...     0    0    0   \n",
      "\n",
      "        9_7  9_8  9_9  9_10  9_11  9_12  9_13  \n",
      "999995    0    0    0     0     0     0     0  \n",
      "999996    0    0    0     0     0     0     0  \n",
      "999997    1    0    0     0     0     0     0  \n",
      "999998    0    1    0     0     0     0     0  \n",
      "999999    0    0    0     0     0     0     0  \n",
      "\n",
      "[5 rows x 86 columns]\n"
     ]
    }
   ],
   "source": [
    "df = read_data('poker-hand-training-true.data')\n",
    "df_test = read_data('poker-hand-testing.data')\n",
    "# df.tail()\n",
    "# df_test.tail()\n",
    "df = one_hot(df, df.iloc[:,:-1].columns)\n",
    "df_test = one_hot(df_test, df_test.iloc[:,:-1].columns)\n",
    "print(df.tail())\n",
    "print(df_test.tail())\n",
    "# df[10] = df[10] - 1\n",
    "# df_test[10] = df_test[10] - 1\n",
    "# print(df.tail())\n",
    "# print(df_test.tail())\n",
    "# df[10].value_counts().sort_index().plot('bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.18544144  0.18841386  0.18745751  0.18869141  0.07297093  0.07158867\n",
      "  0.06959014  0.07084475  0.06992965  0.06884207  0.07226361  0.07182504\n",
      "  0.07246585  0.06942026  0.07108162  0.07155489  0.07070933  0.18863199\n",
      "  0.18733744  0.1855836   0.18845355  0.07307185  0.07091245  0.07060773\n",
      "  0.07037055  0.06833114  0.072095    0.07060773  0.07013321  0.06942026\n",
      "  0.07094629  0.07125072  0.0715211   0.07381113  0.18705685  0.18661465\n",
      "  0.18550239  0.19079198  0.07003145  0.07053998  0.07239845  0.07138594\n",
      "  0.0708786   0.06965806  0.07111545  0.07060773  0.06856966  0.07357607\n",
      "  0.0706416   0.07313913  0.07053998  0.18783694  0.1874175   0.18873101\n",
      "  0.18602939  0.07300457  0.07313913  0.07354248  0.07026885  0.0696241\n",
      "  0.0706416   0.07165622  0.06901222  0.07175752  0.07101396  0.07199379\n",
      "  0.06809246  0.06931829  0.18861217  0.18588772  0.18873101  0.18677563\n",
      "  0.07256692  0.07212873  0.07067547  0.07175752  0.07233104  0.06999752\n",
      "  0.06792188  0.07337447  0.07125072  0.07182504  0.07037055  0.06894417\n",
      "  0.06992965]\n",
      "(25010, 85) (25010, 1)\n"
     ]
    }
   ],
   "source": [
    "features = df.iloc[:, 1:].values\n",
    "labels = df.iloc[:, :1].values\n",
    "print(stats.describe(features).variance)\n",
    "print(features.shape, labels.shape)\n",
    "\n",
    "features_test = df_test.iloc[:, 1:].values\n",
    "labels_test = df_test.iloc[:, :1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 1\n"
     ]
    }
   ],
   "source": [
    "train_x = features\n",
    "train_y = labels\n",
    "test_x = features_test\n",
    "test_y = labels_test\n",
    "\n",
    "feature_count = train_x.shape[1]\n",
    "label_count = train_y.shape[1]\n",
    "print(feature_count, label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot Tensor(\"one_hot_10:0\", shape=(?, 1, 10), dtype=float32)\n",
      "reshape Tensor(\"Reshape_20:0\", shape=(?, 10), dtype=float32)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 2000\n",
    "learning_rate = 0.001\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "nb_classes = 10\n",
    "\n",
    "# x는 float32 로 할 필요가 있나? normalized 되었기때문에 float32 써야함 or dropout에서 float를 씀\n",
    "X = tf.placeholder(tf.float32,[None,feature_count])\n",
    "Y = tf.placeholder(tf.int32,[None,label_count])\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot\", Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape\", Y_one_hot)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(np.array_equal(sess.run(tf.one_hot(train_y, nb_classes)), one_hot(df, df.iloc[:,:1].columns).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.1)), tf.Variable(tf.random_normal([shape[1]]))\n",
    "\n",
    "def make_hidden_layer(previous_h, weight, bias, p_keep_hidden):\n",
    "    h = tf.nn.relu(tf.matmul(previous_h, weight) + bias)\n",
    "#     h = tf.nn.dropout(h, p_keep_hidden)\n",
    "    return h\n",
    "\n",
    "def model(X, p_keep_input, p_keep_hidden): # this network is the same as the previous one except with an extra hidden layer + dropout\n",
    "    s_1 = feature_count + 2\n",
    "    s_2 = feature_count + 2\n",
    "    s_3 = feature_count\n",
    "#     s_4 = feature_count\n",
    "    \n",
    "    w_h, b = init_weights([feature_count, s_1])\n",
    "    w_h2, b2 = init_weights([s_1, s_2])\n",
    "    w_h3, b3 = init_weights([s_2, s_3])\n",
    "#     w_h4, b4 = init_weights([s_3, s_4])\n",
    "    w_o, b_o = init_weights([s_3, nb_classes])\n",
    "    \n",
    "#     X = tf.nn.dropout(X, p_keep_input)\n",
    "    h = make_hidden_layer(X, w_h, b, p_keep_hidden)\n",
    "    h2 = make_hidden_layer(h, w_h2, b2, p_keep_hidden)\n",
    "    h3 = make_hidden_layer(h2, w_h3, b3, p_keep_hidden)\n",
    "#     h4 = make_hidden_layer(h3, w_h4, b4, p_keep_hidden)\n",
    "    \n",
    "    return tf.matmul(h3, w_o) + b_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_keep_input = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "h0 = model(X, p_keep_input, p_keep_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=h0, labels=Y_one_hot))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.argmax(h0, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25010, 85) (25010, 1)\n",
      "(1000000, 85) (1000000, 1)\n",
      "(?, 85) (?, 1)\n",
      "Step:     0\tLoss: 2.691\tAcc: 0.02%\n",
      "Step:   500\tLoss: 0.038\tAcc: 99.19%\n",
      "Step:  1000\tLoss: 0.002\tAcc: 99.99%\n",
      "Step:  1500\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  2000\tLoss: 0.000\tAcc: 100.00%\n",
      "(1000000,)\n",
      "Test Accuracy: 0.972227\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)\n",
    "print(X.shape, Y.shape)\n",
    "training_dropout_i = 0.8\n",
    "training_dropout_h = 0.7\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(training_epochs + 1):\n",
    "        sess.run(optimizer, feed_dict={X: train_x, Y: train_y, p_keep_input: training_dropout_i, p_keep_hidden: training_dropout_h})\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={\n",
    "                                 X: train_x, Y: train_y, p_keep_input: training_dropout_i, p_keep_hidden: training_dropout_h})\n",
    "        cost_history = np.append(cost_history, acc)\n",
    "        if step % 500 == 0:\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(\n",
    "                step, loss, acc))\n",
    "            \n",
    "    # Test model and check accuracy\n",
    "    pre = tf.argmax(h0, 1)\n",
    "    test_yy = np.transpose(test_y.ravel())\n",
    "    print(test_yy.shape)\n",
    "    correct_prediction = tf.equal(pre, test_yy)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('Test Accuracy:', sess.run(accuracy, feed_dict={X: test_x, \n",
    "                                                         p_keep_input: 1.0,\n",
    "                                                         p_keep_hidden: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2002,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAF+lJREFUeJzt3XuQXGWZx/Hv090zk8wkJCEJkCskGIjBQAJDQOWmuEJw\nNeu1EvHG6kZqCYW1tbViWbpuWfsH62qtFkg2qyl0S8FSQaOGy+oqyGKACeQKJEwSkpnck8nkwkwy\nM93P/tFnQmfIzJwJPf329Pl9iqnpc/rNOc+cbs6v3/ecPsfcHRERSZ5U6AJERCQMBYCISEIpAERE\nEkoBICKSUAoAEZGEUgCIiCRUvwFgZsvNbJ+ZbejleTOz75lZo5mtM7PLi1+miIgUW5wewAPAzX08\nPx+YEf0sBu5/62WJiMhg6zcA3P0poKWPJguAH3veKmC0mU0oVoEiIjI4MkVYxiSgqWC6OZq3u2dD\nM1tMvpdAXV3dFTNnzizC6iWunDs5h1zOo8dONsfJx7noMYADHrV392gaHM//jh5H/536XPe6cvnp\nrL5tLlI0HXsaD7j7+GIsqxgBEJu7LwOWAdTX13tDQ0MpV1+23J3mQ+3sam2nriZDZzZHe2eW451Z\njh7v4kh7J8dOZOnoytGRjX535ejI5ujMOtmc05nNkc05x0500daRpb0jS3tnltej6dc7uoizH7aC\n3+mUUZU2qtIpajIpMqkUVRmjKpWiOpMikzbSqRSZlOV/0pZvkzaGVaWprU5TW51h5LAM1ekUmXT+\nufTJ9qmT6yhcTjplpMwwg5RZ9APWy+/uNhYVbwZ28i/p42/tv0msNvntVaT1xVlXrJqKU0+8JeVf\ni+IsJ0abIv5tccR7n5Tu7584unZ7jEXFUowA2AlMKZieHM1LvM5sjp8938S+I8epzqRo68iy5/Bx\nDrd30treyaHXO2ht76Tl9Y7Yy0ynjOp0ipqqFFXpFFXRjrR7x1lbk2FETZrRw6sYXp1mRE2GupoM\nddVpagsfV2eoq8n/HlGTobY6TV1NhpRBdSZFTSZNOlXE/4tEpOwUIwBWAEvM7CHgKuCwu79p+CcJ\ndra289iGPTS1tPHS7iM8t+3Nh06q0ykuPGcEY2qrePuEsxhTV8Xxzhxrmlq5+ZLzmHHuCEYOyzC8\nKhPtwPM76TG11VSl8zt7EZFi6DcAzOxB4AZgnJk1A/8MVAG4+1JgJXAL0Ai0AbcNVrHlaMfBNv79\niU08s+UAB47lP8nXVqeZPr6Omy45l2e3tXDne2fwwUsnMG5EDSl9qhaRMtFvALj7on6ed+COolU0\nRHz/T43822ObTk5PHjOcr8yfyXUXjefic0dqRy8iZa+kB4ErQfOhNq65548npyeMGsY9H72U6y4q\nykF5EZGSUQAMwJ7Dx0/Z+f9myTXMnjwqYEUiImdOARDTviPH+ej9z5ycfvVf51OlA7IiMoQpAGLo\nyub4yP3PcKitgx9+tp4b335u6JJERN4yfYSN4dYfPEvzoXbuunGGdv4iUjEUAP1Yvb2FZ6Pz+Rdf\nNz1wNSIixaMA6Md3/9AIwCN//65YX/cWERkqFAB9WP70Np7avJ+7bpzB3KljQpcjIlJUCoBeuDvL\n/28bAF+8XkM/IlJ5FAC9eG5bC82H2vnS+2ZQW62TpUSk8igAevHpHz5HXXWav7tWn/5FpDIpAE7j\nF6ub6cjmuGTiKOpq9OlfRCqTAuA0nti4B4B7b50buBIRkcGjAOjhcHsnT7y0l1uvmso5I4eFLkdE\nZNAoAHr4zhP5Szx/YLbuay8ilU0B0MMjL+bvZvnOC8cGrkREZHApAAps2X+MI8e7WDRvir71KyIV\nTwFQ4NH1+VsZL7xyauBKREQGnwKgwOMb9zLzvJFcNmV06FJERAadAiCy/eDrrN95mPdfcl7oUkRE\nSkIBEPnuH14F4OrpZweuRESkNBQAkeZD7QBccb6u+ikiyaAAAE50ZVnffJhF86ZQk0mHLkdEpCQU\nAMDq7Ydo78zy3pm63aOIJIcCAHj61QOkU6bxfxFJFAUA8OTm/VwxdQwjh1WFLkVEpGQSHwD7jh5n\n464jXH/x+NCliIiUVOID4KnNBwC4/iIFgIgkS+ID4MnN+xk/soZLJp4VuhQRkZJKdABkc86fX93P\ndTPG6+JvIpI4iQ6Atc2ttLZ1coPG/0UkgRIdAE9u2k/K4Jq3jQtdiohIySU6AP60eT+XTRnNmLrq\n0KWIiJRcYgOg5fUO1jW3csNF54QuRUQkiMQGwNONB3BH5/+LSGLFCgAzu9nMNplZo5ndfZrnR5nZ\nb8xsrZltNLPbil9qcT2/rYXhVWneodM/RSSh+g0AM0sD9wHzgVnAIjOb1aPZHcBL7n4ZcAPwbTMr\n24H1XM55dMNu5k4dTSad2E6QiCRcnL3fPKDR3be6ewfwELCgRxsHRlr+ZPoRQAvQVdRKi2jT3qMc\nONbBh+dOCl2KiEgwcQJgEtBUMN0czSt0L/B2YBewHrjL3XM9F2Rmi82swcwa9u/ff4Ylv3XPbDkI\nwLt1+qeIJFixxj9uAtYAE4E5wL1m9qbBdXdf5u717l4/fny4g69/2XKAC8bWMnH08GA1iIiEFicA\ndgJTCqYnR/MK3QY87HmNwDZgZnFKLK6ubI5nt7bwzgv16V9Eki1OADwPzDCzadGB3YXAih5tdgA3\nApjZucDFwNZiFlosG3cd4eiJLt514djQpYiIBJXpr4G7d5nZEuBxIA0sd/eNZnZ79PxS4JvAA2a2\nHjDgy+5+YBDrPmOrtubH/6+ergAQkWTrNwAA3H0lsLLHvKUFj3cB7y9uaYPj2W0tXDC2lvEja0KX\nIiISVKJOgnd3/veVfbxLZ/+IiCQrABq2HwJgrC7+JiKSrAB4IQqAj1w+OXAlIiLhJSoANuw6wqTR\nw5k2ri50KSIiwSUmANydtU2tuveviEgkMQHQfKidHS1tOv1TRCSSmAD4S3T9nzlTRweuRESkPCQm\nADbtPUpNJsXcKQoAERFIUAA0vNbCOyaNIn/FahERSUQAHG7vZP3Ow7r8s4hIgUQEwKqtB8k5ugCc\niEiBRATA71/ay1nDMszVAWARkZMSEQA/X93M+WPrqMmkQ5ciIlI2Kj4Adh9uB6Ar54ErEREpLxUf\nAH9+NX9bgiXveVvgSkREykvFB8CGnYcBuOHicPcgFhEpRxUfAE9u3s97Lh5PXU2se9+IiCRGRQdA\ny+sdbD/YxlW6/o+IyJtUdACsacpf/1+XfxARebOKDoAXd7SSThmzJ48KXYqISNmp+ACYed5Iaqs1\n/i8i0lPFBkA256xpatW3f0VEelGxAbBl/zGOnehi7pQxoUsRESlLFRsAL+6IDgCrByAicloVGwBr\nmloZNbxKN4AXEelFxQbAiztamTNltG4AIyLSi4oMgGMnuti09yhzdP6/iEivKjIA1jcfxl03gBcR\n6UtFBsCaplYA5kxWAIiI9KYiA2BtUyvnj61lTF116FJERMpWRQbAmqZWjf+LiPSj4gJgz+Hj7Dly\nnMs0/CMi0qeKC4CT4/86ACwi0qeKC4C1za1UpY1ZE84KXYqISFmLFQBmdrOZbTKzRjO7u5c2N5jZ\nGjPbaGZPFrfM+J7f1sKsiaMYVpUOVYKIyJDQbwCYWRq4D5gPzAIWmdmsHm1GA98HPuTulwAfH4Ra\n+9XW0cXa5lbeqTuAiYj0K04PYB7Q6O5b3b0DeAhY0KPNJ4GH3X0HgLvvK26Z8bywvZXOrHP19LND\nrF5EZEiJEwCTgKaC6eZoXqGLgDFm9iczW21mnzndgsxssZk1mFnD/v37z6ziPqzaepB0yqi/QAEg\nItKfYh0EzgBXAB8AbgK+ZmYX9Wzk7svcvd7d68ePH1+kVb9h1daDzJ40ihE1ugOYiEh/4gTATmBK\nwfTkaF6hZuBxd3/d3Q8ATwGXFafEeLrH/6/W+L+ISCxxAuB5YIaZTTOzamAhsKJHm18D15hZxsxq\ngauAl4tbat9Wbz+k8X8RkQHod6zE3bvMbAnwOJAGlrv7RjO7PXp+qbu/bGaPAeuAHPADd98wmIX3\n1D3+f6XG/0VEYok1WO7uK4GVPeYt7TH9LeBbxSttYFZtbeHSyaOo0/i/iEgsFfFN4LaOLtY2afxf\nRGQgKiIAVm8/RFfOFQAiIgNQEQHwly3R+f/njwldiojIkDHkA8Dd+d363cy74GyN/4uIDMCQD4Dn\ntrWw/WAbH7ticuhSRESGlCEfAL9Y3cyImgzzZ58XuhQRkSFlSAfAsRNd/G79bj4wewK11Rr+EREZ\niCEdACvX7aatI8snrpzSf2MRETnFkA6AxzbuYfKY4Vyu2z+KiAzYkA2A/UdP8HTjAf5q1rmYWehy\nRESGnCEbAA8+t4OOrhyfuvr80KWIiAxJQzIAurI5fvrsDq6dMY4Lx48IXY6IyJA0JAPgt+t2s+fI\ncX36FxF5C4ZcAOxqbedrv9rArAlncePMc0KXIyIyZA2pAHhlzxG+8KMGsu4s/dQVZNJDqnwRkbIS\n/NtTv1u3m1VbD5IySKWMbM7pzDpd2Vz+cc450Zml+VA7m/ceZdTwKr67cC5Tx9aGLl1EZEgLGgDH\nO7Pc+eAL5PyNebXVaepqMmRSRiZtVKVSVKVTTBw9jGsvGsfia6czdkRNuKJFRCpE0ABo68iSc/ji\nddP5z6e2ArDxX27Sef0iIiUQdBD9RFcWgAvG1Z2cp52/iEhpBB4CygEwrCrFr+54N2PrqkOWIyKS\nKEEDoLsHUJNJM2eKrucjIlJKYYeACnoAIiJSWkH3vMc73+gBiIhIaQU+CKwegIhIKOoBiIgklHoA\nIiIJVRYBoB6AiEjplckQkHoAIiKlVh49gCr1AERESk09ABGRhCqPHoACQESk5ILueXe1tlOdSekC\ncCIiAQQLgF2t7fxidTPnn60bu4iIhBAsALqHf7718ctClSAikmhBh4DmTh2tq4CKiAQSKwDM7GYz\n22RmjWZ2dx/trjSzLjP7WKzlxq1SRESKrt8AMLM0cB8wH5gFLDKzWb20uwd4Iu7KdfBXRCScOD2A\neUCju2919w7gIWDBadrdCfwS2Bd35dr9i4iEEycAJgFNBdPN0byTzGwS8GHg/r4WZGaLzazBzBo6\nOjoGWquIiBRRsQ4C/wfwZXfP9dXI3Ze5e72711dVVaMRIBGRcOLcE3gnMKVgenI0r1A98FA0pj8O\nuMXMutz9V30t2DQIJCISTJwAeB6YYWbTyO/4FwKfLGzg7tO6H5vZA8Bv+9v55xsPpFQRESmmfgPA\n3bvMbAnwOJAGlrv7RjO7PXp+6ZmuXPt/EZFw4vQAcPeVwMoe806743f3z8VduY4BiIiEE/CbwK5j\nACIiAQULAEc9ABGRkIJeC0gBICISTtgA0BCQiEgw6gGIiCRUuGMAHmrNIiICwXsA6gKIiIQS+BiA\niIiEomMAIiIJFTQAREQkHA0BiYgklA4Ci4gkVNhLQYRauYiIhO4BhFy7iEiyBT4IrAQQEQklXAC4\negAiIiHpLCARkYTSMQARkYQKeBaQ7ggmIhKSegAiIgmlABARSSjdEUxEJKHCfg9A+38RkWB0GqiI\nSEKFvRaQDgKIiASj+wGIiCRU2EtBBFu5iIjoNFARkYTSQWARkYTSHcFERBJKdwQTEUkofRFMRCSh\ndCkIEZGE0llAIiIJFSsAzOxmM9tkZo1mdvdpnr/VzNaZ2Xoze8bMLou13IFWKyIiRdNvAJhZGrgP\nmA/MAhaZ2awezbYB17v7bOCbwLJ+16x7AouIBBWnBzAPaHT3re7eATwELChs4O7PuPuhaHIVMLm/\nheqOYCIiYcUJgElAU8F0czSvN58HHj3dE2a22MwazKwhl8upByAiElBRDwKb2XvIB8CXT/e8uy9z\n93p3r0+lUgoAEZGAMjHa7ASmFExPjuadwswuBX4AzHf3g/FWrwQQEQklTg/geWCGmU0zs2pgIbCi\nsIGZTQUeBj7t7pvjrlw9ABGRcPrtAbh7l5ktAR4H0sByd99oZrdHzy8Fvg6MBb4fXd+ny93rB69s\nERF5q+IMAeHuK4GVPeYtLXj8BeALA125OgAiIuEEviVkqLWLiIiuBSQiklC6FpCISELpnsAiIgml\nO4KJiCRU0IPAIiISjo4BiIgklM4CEhFJKPUAREQSKnAPQEREQlEPQEQkoQKeBeQ6DVREJCB9EUxE\nJKGCDgEpAUREwgkbACIiEoy+ByAiklA6C0hEJKH0PQARkYTSHcFERBJKxwBERBJKxwBERBJKxwBE\nRBIq8BfBFAEiIqGoByAiklA6BiAiklA6C0hEJKHUAxARSSgdAxARSSj1AEREEipwACgBRERC0f0A\nREQSSgEgIpJQOgYgIpJQ+h6AiEhCqQcgIpJQsQLAzG42s01m1mhmd5/meTOz70XPrzOzy2Mtd6DV\niohI0fQbAGaWBu4D5gOzgEVmNqtHs/nAjOhnMXB/nJVn0joGLSISSpw98Dyg0d23unsH8BCwoEeb\nBcCPPW8VMNrMJvS34Emjhw24YBERKY5MjDaTgKaC6WbgqhhtJgG7CxuZ2WLyPQSAE/NnT9wwoGrD\nGAccCF1EDKqzuIZCnUOhRlCdxXZxsRYUJwCKxt2XAcsAzKzB3etLuf4zoTqLS3UWz1CoEVRnsZlZ\nQ7GWFWcIaCcwpWB6cjRvoG1ERKSMxAmA54EZZjbNzKqBhcCKHm1WAJ+Jzga6Gjjs7rt7LkhERMpH\nv0NA7t5lZkuAx4E0sNzdN5rZ7dHzS4GVwC1AI9AG3BZj3cvOuOrSUp3FpTqLZyjUCKqz2IpWp7l7\nsZYlIiJDiE7EFxFJKAWAiEhCBQmA/i4tUcI6ppjZH83sJTPbaGZ3RfO/YWY7zWxN9HNLwb/5SlT3\nJjO7qYS1vmZm66N6GqJ5Z5vZ/5jZq9HvMSHrNLOLC7bZGjM7YmZfKoftaWbLzWyfmW0omDfg7Wdm\nV0SvQ2N0+ZOiXtGklzq/ZWavRJdZecTMRkfzLzCz9oLtujRwnQN+nQPV+bOCGl8zszXR/CDbs4/9\n0OC/P929pD/kDyRvAaYD1cBaYFap64hqmQBcHj0eCWwmf7mLbwD/eJr2s6J6a4Bp0d+RLlGtrwHj\nesz7N+Du6PHdwD2h6+zxOu8Bzi+H7QlcB1wObHgr2w94Dria/KWsHgXml6DO9wOZ6PE9BXVeUNiu\nx3JC1Dng1zlEnT2e/zbw9ZDbk973Q4P+/gzRA4hzaYmScPfd7v5C9Pgo8DL5bzD3ZgHwkLufcPdt\n5M96mjf4lfZZz4+ixz8C/qZgfug6bwS2uPv2PtqUrE53fwpoOc36Y28/y1/e5Cx3X+X5/9t+XPBv\nBq1Od3/C3buiyVXkv2fTq1B19qGstme36NPxJ4AH+1rGYNfZx35o0N+fIQKgt8tGBGVmFwBzgWej\nWXdGXe7lBV2vkLU78HszW235S2oAnOtvfN9iD3Bu9LgctvFCTv0fq9y2Jwx8+02KHvecX0p/S/6T\nXbdp0XDFk2Z2bTQvZJ0DeZ1Db89rgb3u/mrBvKDbs8d+aNDfnzoIDJjZCOCXwJfc/Qj5q5lOB+aQ\nv57RtwOW1+0ad59D/sqrd5jZdYVPRolfFuf0Wv4Lgx8Cfh7NKsfteYpy2n69MbOvAl3AT6JZu4Gp\n0fviH4CfmtlZoepjCLzOPSzi1A8pQbfnafZDJw3W+zNEAJTVZSPMrIr8Rv+Juz8M4O573T3r7jng\nv3hjWCJY7e6+M/q9D3gkqmlv1O3r7qbuC11nZD7wgrvvhfLcnpGBbr+dnDr8UrJ6zexzwF8Dt0Y7\nA6IhgIPR49Xkx4IvClXnGbzOIbdnBvgI8LPueSG35+n2Q5Tg/RkiAOJcWqIkojHAHwIvu/t3CuYX\nXsr6w0D3GQQrgIVmVmNm08jf/+C5EtRZZ2Yjux+TPyi4Iarns1GzzwK/DllngVM+WZXb9iwwoO0X\ndcePmNnV0XvnMwX/ZtCY2c3APwEfcve2gvnjLX+/DsxselTn1oB1Duh1DlVn5H3AK+5+csgk1Pbs\nbT9EKd6fxTqSPcCj3reQP9K9BfhqiBqiOq4h361aB6yJfm4B/htYH81fAUwo+DdfjereRJHPWOij\nzunkj/qvBTZ2bzNgLPAH4FXg98DZIeuM1lsHHARGFcwLvj3JB9JuoJP82Ojnz2T7AfXkd2xbgHuJ\nvk0/yHU2kh/z7X6PLo3afjR6P6wBXgA+GLjOAb/OIeqM5j8A3N6jbZDtSe/7oUF/f+pSECIiCaWD\nwCIiCaUAEBFJKAWAiEhCKQBERBJKASAiklAKABGRhFIAiIgk1P8DtXoUIByJksAAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27a8197b438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(cost_history.shape)\n",
    "plt.plot(range(len(cost_history)),cost_history)\n",
    "plt.axis([0,training_epochs,0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
