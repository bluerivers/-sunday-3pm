{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import genfromtxt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is z-score that value minus mean divided by standard deviation\n",
    "# http://duramecho.com/Misc/WhyMinusOneInSd.html\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis=0)\n",
    "    sigma = np.std(dataset,axis=0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "def append_bias_reshape(features,labels):\n",
    "    n_training_samples = features.shape[0]\n",
    "    n_dim = features.shape[1]\n",
    "    f = np.c_[np.ones(n_training_samples),features],[n_training_samples,n_dim + 1]\n",
    "    l = np.reshape(labels,[n_training_samples,1])\n",
    "    return f, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    df = pd.read_csv(file_name, sep=',', header=None)\n",
    "    return df\n",
    "\n",
    "def merge_column(df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in range(df.shape[1]/2):\n",
    "        new_df[i] = df[i] * df[i+1]\n",
    "    return new_df\n",
    "\n",
    "# https://stackoverflow.com/a/42523230\n",
    "def one_hot(df, cols):\n",
    "    \"\"\"\n",
    "    @param df pandas DataFrame\n",
    "    @param cols a list of columns to encode \n",
    "    @return a DataFrame with one-hot encoding\n",
    "    \"\"\"\n",
    "    for each in cols:\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        del df[each]\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       10  0_1  0_2  0_3  0_4  1_1  1_2  1_3  1_4  1_5  ...   9_4  9_5  9_6  \\\n",
      "25005   0    0    0    1    0    0    0    0    0    0  ...     1    0    0   \n",
      "25006   1    0    0    0    1    1    0    0    0    0  ...     0    0    0   \n",
      "25007   1    0    1    0    0    1    0    0    0    0  ...     0    0    0   \n",
      "25008   1    0    1    0    0    0    0    0    0    0  ...     0    0    0   \n",
      "25009   1    1    0    0    0    0    0    0    0    0  ...     0    0    0   \n",
      "\n",
      "       9_7  9_8  9_9  9_10  9_11  9_12  9_13  \n",
      "25005    0    0    0     0     0     0     0  \n",
      "25006    0    0    0     1     0     0     0  \n",
      "25007    0    0    0     0     0     0     1  \n",
      "25008    0    0    1     0     0     0     0  \n",
      "25009    1    0    0     0     0     0     0  \n",
      "\n",
      "[5 rows x 86 columns]\n",
      "        10  0_1  0_2  0_3  0_4  1_1  1_2  1_3  1_4  1_5  ...   9_4  9_5  9_6  \\\n",
      "999995   1    0    0    1    0    1    0    0    0    0  ...     0    0    1   \n",
      "999996   1    0    0    1    0    0    0    1    0    0  ...     0    0    0   \n",
      "999997   1    1    0    0    0    0    0    0    0    0  ...     0    0    0   \n",
      "999998   1    0    0    1    0    0    0    0    0    0  ...     0    0    0   \n",
      "999999   2    0    1    0    0    0    0    0    0    1  ...     0    0    0   \n",
      "\n",
      "        9_7  9_8  9_9  9_10  9_11  9_12  9_13  \n",
      "999995    0    0    0     0     0     0     0  \n",
      "999996    0    0    0     0     0     0     0  \n",
      "999997    1    0    0     0     0     0     0  \n",
      "999998    0    1    0     0     0     0     0  \n",
      "999999    0    0    0     0     0     0     0  \n",
      "\n",
      "[5 rows x 86 columns]\n"
     ]
    }
   ],
   "source": [
    "df = read_data('poker-hand-training-true.data')\n",
    "df_test = read_data('poker-hand-testing.data')\n",
    "# df.tail()\n",
    "# df_test.tail()\n",
    "df = one_hot(df, df.iloc[:,:-1].columns)\n",
    "df_test = one_hot(df_test, df_test.iloc[:,:-1].columns)\n",
    "print(df.tail())\n",
    "print(df_test.tail())\n",
    "# df[10] = df[10] - 1\n",
    "# df_test[10] = df_test[10] - 1\n",
    "# print(df.tail())\n",
    "# print(df_test.tail())\n",
    "# df[10].value_counts().sort_index().plot('bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.18544144  0.18841386  0.18745751  0.18869141  0.07297093  0.07158867\n",
      "  0.06959014  0.07084475  0.06992965  0.06884207  0.07226361  0.07182504\n",
      "  0.07246585  0.06942026  0.07108162  0.07155489  0.07070933  0.18863199\n",
      "  0.18733744  0.1855836   0.18845355  0.07307185  0.07091245  0.07060773\n",
      "  0.07037055  0.06833114  0.072095    0.07060773  0.07013321  0.06942026\n",
      "  0.07094629  0.07125072  0.0715211   0.07381113  0.18705685  0.18661465\n",
      "  0.18550239  0.19079198  0.07003145  0.07053998  0.07239845  0.07138594\n",
      "  0.0708786   0.06965806  0.07111545  0.07060773  0.06856966  0.07357607\n",
      "  0.0706416   0.07313913  0.07053998  0.18783694  0.1874175   0.18873101\n",
      "  0.18602939  0.07300457  0.07313913  0.07354248  0.07026885  0.0696241\n",
      "  0.0706416   0.07165622  0.06901222  0.07175752  0.07101396  0.07199379\n",
      "  0.06809246  0.06931829  0.18861217  0.18588772  0.18873101  0.18677563\n",
      "  0.07256692  0.07212873  0.07067547  0.07175752  0.07233104  0.06999752\n",
      "  0.06792188  0.07337447  0.07125072  0.07182504  0.07037055  0.06894417\n",
      "  0.06992965]\n",
      "(25010, 85) (25010, 1)\n"
     ]
    }
   ],
   "source": [
    "features = df.iloc[:, 1:].values\n",
    "labels = df.iloc[:, :1].values\n",
    "print(stats.describe(features).variance)\n",
    "print(features.shape, labels.shape)\n",
    "\n",
    "features_test = df_test.iloc[:, 1:].values\n",
    "labels_test = df_test.iloc[:, :1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 1\n"
     ]
    }
   ],
   "source": [
    "train_x = features\n",
    "train_y = labels\n",
    "test_x = features_test\n",
    "test_y = labels_test\n",
    "\n",
    "feature_count = train_x.shape[1]\n",
    "label_count = train_y.shape[1]\n",
    "print(feature_count, label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot Tensor(\"one_hot_8:0\", shape=(?, 1, 10), dtype=float32)\n",
      "reshape Tensor(\"Reshape_16:0\", shape=(?, 10), dtype=float32)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 9000\n",
    "learning_rate = 0.001\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "nb_classes = 10\n",
    "\n",
    "# x는 float32 로 할 필요가 있나? normalized 되었기때문에 float32 써야함 or dropout에서 float를 씀\n",
    "X = tf.placeholder(tf.float32,[None,feature_count])\n",
    "Y = tf.placeholder(tf.int32,[None,label_count])\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot\", Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape\", Y_one_hot)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(np.array_equal(sess.run(tf.one_hot(train_y, nb_classes)), one_hot(df, df.iloc[:,:1].columns).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.1)), tf.Variable(tf.random_normal([shape[1]]))\n",
    "\n",
    "def make_hidden_layer(previous_h, weight, bias, p_keep_hidden):\n",
    "    h = tf.nn.relu(tf.matmul(previous_h, weight) + bias)\n",
    "    h = tf.nn.dropout(h, p_keep_hidden)\n",
    "    return h\n",
    "\n",
    "def model(X, p_keep_input, p_keep_hidden): # this network is the same as the previous one except with an extra hidden layer + dropout\n",
    "    s_1 = feature_count + 2\n",
    "    s_2 = feature_count + 2\n",
    "    s_3 = feature_count\n",
    "#     s_4 = feature_count\n",
    "    \n",
    "    w_h, b = init_weights([feature_count, s_1])\n",
    "    w_h2, b2 = init_weights([s_1, s_2])\n",
    "    w_h3, b3 = init_weights([s_2, s_3])\n",
    "#     w_h4, b4 = init_weights([s_3, s_4])\n",
    "    w_o, b_o = init_weights([s_3, nb_classes])\n",
    "    \n",
    "    X = tf.nn.dropout(X, p_keep_input)\n",
    "    h = make_hidden_layer(X, w_h, b, p_keep_hidden)\n",
    "    h2 = make_hidden_layer(h, w_h2, b2, p_keep_hidden)\n",
    "    h3 = make_hidden_layer(h2, w_h3, b3, p_keep_hidden)\n",
    "#     h4 = make_hidden_layer(h3, w_h4, b4, p_keep_hidden)\n",
    "    \n",
    "    return tf.matmul(h3, w_o) + b_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_keep_input = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "h0 = model(X, p_keep_input, p_keep_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=h0, labels=Y_one_hot))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.argmax(h0, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25010, 85) (25010, 1)\n",
      "(1000000, 85) (1000000, 1)\n",
      "(?, 85) (?, 1)\n",
      "Step:     0\tLoss: 2.781\tAcc: 11.54%\n",
      "Step:   500\tLoss: 0.774\tAcc: 67.10%\n",
      "Step:  1000\tLoss: 0.647\tAcc: 73.63%\n",
      "Step:  1500\tLoss: 0.581\tAcc: 76.29%\n",
      "Step:  2000\tLoss: 0.557\tAcc: 77.17%\n",
      "Step:  2500\tLoss: 0.539\tAcc: 77.91%\n",
      "Step:  3000\tLoss: 0.520\tAcc: 78.79%\n",
      "Step:  3500\tLoss: 0.513\tAcc: 78.58%\n",
      "Step:  4000\tLoss: 0.509\tAcc: 78.87%\n",
      "Step:  4500\tLoss: 0.498\tAcc: 79.19%\n",
      "Step:  5000\tLoss: 0.492\tAcc: 79.33%\n",
      "Step:  5500\tLoss: 0.490\tAcc: 79.21%\n",
      "Step:  6000\tLoss: 0.484\tAcc: 79.30%\n",
      "Step:  6500\tLoss: 0.483\tAcc: 79.79%\n",
      "Step:  7000\tLoss: 0.486\tAcc: 79.44%\n",
      "Step:  7500\tLoss: 0.481\tAcc: 79.80%\n",
      "Step:  8000\tLoss: 0.479\tAcc: 79.55%\n",
      "Step:  8500\tLoss: 0.478\tAcc: 79.70%\n",
      "Step:  9000\tLoss: 0.475\tAcc: 80.01%\n",
      "(1000000,)\n",
      "Test Accuracy: 0.923707\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)\n",
    "print(X.shape, Y.shape)\n",
    "training_dropout_i = 0.8\n",
    "training_dropout_h = 0.7\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(training_epochs + 1):\n",
    "        sess.run(optimizer, feed_dict={X: train_x, Y: train_y, p_keep_input: training_dropout_i, p_keep_hidden: training_dropout_h})\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={\n",
    "                                 X: train_x, Y: train_y, p_keep_input: training_dropout_i, p_keep_hidden: training_dropout_h})\n",
    "        cost_history = np.append(cost_history, acc)\n",
    "        if step % 500 == 0:\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(\n",
    "                step, loss, acc))\n",
    "            \n",
    "    # Test model and check accuracy\n",
    "    pre = tf.argmax(h0, 1)\n",
    "    test_yy = np.transpose(test_y.ravel())\n",
    "    print(test_yy.shape)\n",
    "    correct_prediction = tf.equal(pre, test_yy)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('Test Accuracy:', sess.run(accuracy, feed_dict={X: test_x, \n",
    "                                                         p_keep_input: 1.0,\n",
    "                                                         p_keep_hidden: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9002,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x238a9d47ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(cost_history.shape)\n",
    "plt.plot(range(len(cost_history)),cost_history)\n",
    "plt.axis([0,training_epochs,0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
