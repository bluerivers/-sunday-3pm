{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import genfromtxt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is z-score that value minus mean divided by standard deviation\n",
    "# http://duramecho.com/Misc/WhyMinusOneInSd.html\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis=0)\n",
    "    sigma = np.std(dataset,axis=0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "def append_bias_reshape(features,labels):\n",
    "    n_training_samples = features.shape[0]\n",
    "    n_dim = features.shape[1]\n",
    "    f = np.c_[np.ones(n_training_samples),features],[n_training_samples,n_dim + 1]\n",
    "    l = np.reshape(labels,[n_training_samples,1])\n",
    "    return f, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    df = pd.read_csv(file_name, sep=',', header=None)\n",
    "    return df\n",
    "\n",
    "def merge_column(df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in range(df.shape[1]/2):\n",
    "        new_df[i] = df[i] * df[i+1]\n",
    "    return new_df\n",
    "\n",
    "# https://stackoverflow.com/a/42523230\n",
    "def one_hot(df, cols):\n",
    "    \"\"\"\n",
    "    @param df pandas DataFrame\n",
    "    @param cols a list of columns to encode \n",
    "    @return a DataFrame with one-hot encoding\n",
    "    \"\"\"\n",
    "    for each in cols:\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        del df[each]\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       10  0_1  0_2  0_3  0_4  1_1  1_2  1_3  1_4  1_5  ...   9_4  9_5  9_6  \\\n",
      "25005   0    0    0    1    0    0    0    0    0    0  ...     1    0    0   \n",
      "25006   1    0    0    0    1    1    0    0    0    0  ...     0    0    0   \n",
      "25007   1    0    1    0    0    1    0    0    0    0  ...     0    0    0   \n",
      "25008   1    0    1    0    0    0    0    0    0    0  ...     0    0    0   \n",
      "25009   1    1    0    0    0    0    0    0    0    0  ...     0    0    0   \n",
      "\n",
      "       9_7  9_8  9_9  9_10  9_11  9_12  9_13  \n",
      "25005    0    0    0     0     0     0     0  \n",
      "25006    0    0    0     1     0     0     0  \n",
      "25007    0    0    0     0     0     0     1  \n",
      "25008    0    0    1     0     0     0     0  \n",
      "25009    1    0    0     0     0     0     0  \n",
      "\n",
      "[5 rows x 86 columns]\n",
      "        10  0_1  0_2  0_3  0_4  1_1  1_2  1_3  1_4  1_5  ...   9_4  9_5  9_6  \\\n",
      "999995   1    0    0    1    0    1    0    0    0    0  ...     0    0    1   \n",
      "999996   1    0    0    1    0    0    0    1    0    0  ...     0    0    0   \n",
      "999997   1    1    0    0    0    0    0    0    0    0  ...     0    0    0   \n",
      "999998   1    0    0    1    0    0    0    0    0    0  ...     0    0    0   \n",
      "999999   2    0    1    0    0    0    0    0    0    1  ...     0    0    0   \n",
      "\n",
      "        9_7  9_8  9_9  9_10  9_11  9_12  9_13  \n",
      "999995    0    0    0     0     0     0     0  \n",
      "999996    0    0    0     0     0     0     0  \n",
      "999997    1    0    0     0     0     0     0  \n",
      "999998    0    1    0     0     0     0     0  \n",
      "999999    0    0    0     0     0     0     0  \n",
      "\n",
      "[5 rows x 86 columns]\n"
     ]
    }
   ],
   "source": [
    "df = read_data('poker-hand-training-true.data')\n",
    "df_test = read_data('poker-hand-testing.data')\n",
    "# df.tail()\n",
    "# df_test.tail()\n",
    "df = one_hot(df, df.iloc[:,:-1].columns)\n",
    "df_test = one_hot(df_test, df_test.iloc[:,:-1].columns)\n",
    "print(df.tail())\n",
    "print(df_test.tail())\n",
    "# df[10] = df[10] - 1\n",
    "# df_test[10] = df_test[10] - 1\n",
    "# print(df.tail())\n",
    "# print(df_test.tail())\n",
    "# df[10].value_counts().sort_index().plot('bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.18544144  0.18841386  0.18745751  0.18869141  0.07297093  0.07158867\n",
      "  0.06959014  0.07084475  0.06992965  0.06884207  0.07226361  0.07182504\n",
      "  0.07246585  0.06942026  0.07108162  0.07155489  0.07070933  0.18863199\n",
      "  0.18733744  0.1855836   0.18845355  0.07307185  0.07091245  0.07060773\n",
      "  0.07037055  0.06833114  0.072095    0.07060773  0.07013321  0.06942026\n",
      "  0.07094629  0.07125072  0.0715211   0.07381113  0.18705685  0.18661465\n",
      "  0.18550239  0.19079198  0.07003145  0.07053998  0.07239845  0.07138594\n",
      "  0.0708786   0.06965806  0.07111545  0.07060773  0.06856966  0.07357607\n",
      "  0.0706416   0.07313913  0.07053998  0.18783694  0.1874175   0.18873101\n",
      "  0.18602939  0.07300457  0.07313913  0.07354248  0.07026885  0.0696241\n",
      "  0.0706416   0.07165622  0.06901222  0.07175752  0.07101396  0.07199379\n",
      "  0.06809246  0.06931829  0.18861217  0.18588772  0.18873101  0.18677563\n",
      "  0.07256692  0.07212873  0.07067547  0.07175752  0.07233104  0.06999752\n",
      "  0.06792188  0.07337447  0.07125072  0.07182504  0.07037055  0.06894417\n",
      "  0.06992965]\n",
      "(25010, 85) (25010, 1)\n"
     ]
    }
   ],
   "source": [
    "features = df.iloc[:, 1:].values\n",
    "labels = df.iloc[:, :1].values\n",
    "print(stats.describe(features).variance)\n",
    "print(features.shape, labels.shape)\n",
    "\n",
    "features_test = df_test.iloc[:, 1:].values\n",
    "labels_test = df_test.iloc[:, :1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 1\n"
     ]
    }
   ],
   "source": [
    "train_x = features\n",
    "train_y = labels\n",
    "test_x = features_test\n",
    "test_y = labels_test\n",
    "\n",
    "feature_count = train_x.shape[1]\n",
    "label_count = train_y.shape[1]\n",
    "print(feature_count, label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot Tensor(\"one_hot_8:0\", shape=(?, 1, 10), dtype=float32)\n",
      "reshape Tensor(\"Reshape_16:0\", shape=(?, 10), dtype=float32)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 9000\n",
    "learning_rate = 0.001\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "nb_classes = 10\n",
    "\n",
    "# x는 float32 로 할 필요가 있나? normalized 되었기때문에 float32 써야함 or dropout에서 float를 씀\n",
    "X = tf.placeholder(tf.float32,[None,feature_count])\n",
    "Y = tf.placeholder(tf.int32,[None,label_count])\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot\", Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape\", Y_one_hot)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(np.array_equal(sess.run(tf.one_hot(train_y, nb_classes)), one_hot(df, df.iloc[:,:1].columns).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.1)), tf.Variable(tf.random_normal([shape[1]]))\n",
    "\n",
    "def make_hidden_layer(previous_h, weight, bias, p_keep_hidden):\n",
    "    h = tf.nn.relu(tf.matmul(previous_h, weight) + bias)\n",
    "#     h = tf.nn.dropout(h, p_keep_hidden)\n",
    "    return h\n",
    "\n",
    "def model(X, p_keep_input, p_keep_hidden): # this network is the same as the previous one except with an extra hidden layer + dropout\n",
    "    s_1 = feature_count + 2\n",
    "    s_2 = feature_count + 2\n",
    "    s_3 = feature_count\n",
    "#     s_4 = feature_count\n",
    "    \n",
    "    w_h, b = init_weights([feature_count, s_1])\n",
    "    w_h2, b2 = init_weights([s_1, s_2])\n",
    "    w_h3, b3 = init_weights([s_2, s_3])\n",
    "#     w_h4, b4 = init_weights([s_3, s_4])\n",
    "    w_o, b_o = init_weights([s_3, nb_classes])\n",
    "    \n",
    "#     X = tf.nn.dropout(X, p_keep_input)\n",
    "    h = make_hidden_layer(X, w_h, b, p_keep_hidden)\n",
    "    h2 = make_hidden_layer(h, w_h2, b2, p_keep_hidden)\n",
    "    h3 = make_hidden_layer(h2, w_h3, b3, p_keep_hidden)\n",
    "#     h4 = make_hidden_layer(h3, w_h4, b4, p_keep_hidden)\n",
    "    \n",
    "    return tf.matmul(h3, w_o) + b_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_keep_input = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "h0 = model(X, p_keep_input, p_keep_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=h0, labels=Y_one_hot))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.argmax(h0, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25010, 85) (25010, 1)\n",
      "(1000000, 85) (1000000, 1)\n",
      "(?, 85) (?, 1)\n",
      "Step:     0\tLoss: 2.227\tAcc: 0.14%\n",
      "Step:   500\tLoss: 0.054\tAcc: 99.13%\n",
      "Step:  1000\tLoss: 0.007\tAcc: 99.92%\n",
      "Step:  1500\tLoss: 0.001\tAcc: 100.00%\n",
      "Step:  2000\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  2500\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  3000\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  3500\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  4000\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  4500\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  5000\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  5500\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  6000\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  6500\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  7000\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  7500\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  8000\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  8500\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  9000\tLoss: 0.000\tAcc: 100.00%\n",
      "(1000000,)\n",
      "Test Accuracy: 0.962167\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)\n",
    "print(X.shape, Y.shape)\n",
    "training_dropout_i = 0.8\n",
    "training_dropout_h = 0.7\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(training_epochs + 1):\n",
    "        sess.run(optimizer, feed_dict={X: train_x, Y: train_y, p_keep_input: training_dropout_i, p_keep_hidden: training_dropout_h})\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={\n",
    "                                 X: train_x, Y: train_y, p_keep_input: training_dropout_i, p_keep_hidden: training_dropout_h})\n",
    "        cost_history = np.append(cost_history, acc)\n",
    "        if step % 500 == 0:\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(\n",
    "                step, loss, acc))\n",
    "            \n",
    "    # Test model and check accuracy\n",
    "    pre = tf.argmax(h0, 1)\n",
    "    test_yy = np.transpose(test_y.ravel())\n",
    "    print(test_yy.shape)\n",
    "    correct_prediction = tf.equal(pre, test_yy)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('Test Accuracy:', sess.run(accuracy, feed_dict={X: test_x, \n",
    "                                                         p_keep_input: 1.0,\n",
    "                                                         p_keep_hidden: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9002,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27a939e5048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(cost_history.shape)\n",
    "plt.plot(range(len(cost_history)),cost_history)\n",
    "plt.axis([0,training_epochs,0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
