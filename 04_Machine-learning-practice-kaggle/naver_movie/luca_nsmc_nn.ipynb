{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deera\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from gensim.models import doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec.load('doc2vec_nsmc.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x2082cf6cc18>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('공포영화/Noun', 0.5339546799659729),\n",
       " ('스릴러/Noun', 0.5142008662223816),\n",
       " ('미스터리/Noun', 0.5078734159469604),\n",
       " ('서스펜스/Noun', 0.43593716621398926),\n",
       " ('호러/Noun', 0.43546515703201294),\n",
       " ('공포물/Noun', 0.41218090057373047),\n",
       " ('귀신/Noun', 0.40097883343696594),\n",
       " ('미스테리/Noun', 0.39496350288391113),\n",
       " ('호러영화/Noun', 0.37969350814819336),\n",
       " ('SF/Alpha', 0.37460654973983765)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('공포/Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"ratings_train.txt\",\n",
    "                    header=0,\n",
    "                    delimiter=\"\\t\",\n",
    "                    quoting=3)\n",
    "test = pd.read_csv(\"ratings_test.txt\",\n",
    "                    header=0,\n",
    "                    delimiter=\"\\t\",\n",
    "                    quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          0\n",
      "document    0\n",
      "label       0\n",
      "dtype: int64\n",
      "id          0\n",
      "document    0\n",
      "label       0\n",
      "dtype: int64\n",
      "train (149995, 3), test (49997, 3)\n"
     ]
    }
   ],
   "source": [
    "train.dropna(axis=0, how='any', inplace=True)\n",
    "test.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "print(train.isnull().sum())\n",
    "print(test.isnull().sum())\n",
    "\n",
    "print(\"train %s, test %s\" %\n",
    "      (train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 149995/149995 [02:07<00:00, 1172.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 49997/49997 [00:42<00:00, 1173.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "pos_tagger = Twitter()\n",
    "\n",
    "def tokenize(doc):\n",
    "    # norm, stem은 optional\n",
    "    try:\n",
    "        return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]\n",
    "    except:\n",
    "        print(doc)\n",
    "\n",
    "train_docs = [(tokenize(row[1]), row[2]) for row in tqdm(train.values)]\n",
    "test_docs = [(tokenize(row[1]), row[2]) for row in tqdm(test.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 149995/149995 [00:00<00:00, 356596.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 49997/49997 [00:00<00:00, 256457.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 149995/149995 [00:34<00:00, 4288.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 49997/49997 [00:11<00:00, 4443.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "# 여기서는 15만개 training documents 전부 사용함\n",
    "tagged_train_docs = [TaggedDocument(d, [c]) for d, c in tqdm(train_docs)]\n",
    "tagged_test_docs = [TaggedDocument(d, [c]) for d, c in tqdm(test_docs)]\n",
    "\n",
    "train_x = [model.infer_vector(doc.words) for doc in tqdm(tagged_train_docs)]\n",
    "train_y = [doc.tags[0] for doc in tagged_train_docs]\n",
    "\n",
    "test_x = [model.infer_vector(doc.words) for doc in tqdm(tagged_test_docs)]\n",
    "test_y = [doc.tags[0] for doc in tagged_test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering: 668 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "word_vectors = model.wv.vectors\n",
    "num_clusters = word_vectors.shape[0] // 5\n",
    "\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: %d seconds\" % elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kmean과 word2vec index(word) 합치기\n",
    "word_centroid_map = dict(zip(model.wv.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['일부/Noun', '산다/Noun', '주먹/Noun', '뿌리/Noun', '해대/Noun', '지하/Noun', '벌이/Noun', '부시/Noun', '누리/Noun', '설치/Noun']\n",
      "\n",
      "Cluster 1\n",
      "['만약/Noun', '도무지/Noun', '절대로/Noun', '조차/Noun', '차마/Noun', '고로/Noun', '눈뜨다/Verb', '내/VerbPrefix', '보장/Noun', '한일/Noun', '자가/Noun', '동의/Noun', '은희/Noun', '사정/Noun', '장담/Noun', '위인/Noun', '대중성/Noun', '한눈/Noun', '갈아타다/Verb', '쉴드/Noun', '망정/Noun', '알아듣다/Verb', '사족/Noun', '갈피/Noun', '잠도/Noun', '만도/Noun', '백년/Noun', '본전/Noun', '당분간/Noun', '가늠/Noun', '그러기에/Conjunction', '대항/Noun', '생기다/Adjective', '타국/Noun']\n",
      "\n",
      "Cluster 2\n",
      "['차갑다/Adjective', '덩어리/Noun', '인척/Noun', '걸치다/Verb', '불안/Noun']\n",
      "\n",
      "Cluster 3\n",
      "['용다/Verb', '밝다/Verb', '에서만/Josa', '상이/Noun', '흐뭇하다/Adjective', '하듯/Josa', '최대한/Noun', '런닝/Noun', '만이라도/Josa', '라기보다는/Josa', '열악하다/Adjective']\n",
      "\n",
      "Cluster 4\n",
      "['어딘가/Noun', '흔적/Noun', '내기/Noun', '그려지다/Verb', '치기/Noun', '미술/Noun', '현대인/Noun', '여실히/Noun', '이라기/Josa', '잡기/Noun', '정성/Noun', '쌀/Noun', '두기/Noun']\n",
      "\n",
      "Cluster 5\n",
      "['경악/Noun', '우롱/Noun', '출현/Noun', '조명/Noun', '퇴보/Noun', '섭외/Noun', '부여/Noun', '아이러니/Noun', '가상/Noun', '은퇴/Noun', '추락/Noun', '타락/Noun', '차지/Noun', '공존/Noun', '멸망/Noun', '요구/Noun', '고려/Noun', '공격/Noun', '농락/Noun', '의아/Noun', '어필/Noun', '동원/Noun', '조롱/Noun', '마주/Noun', '학대/Noun', '살해/Noun', '배제/Noun', '도배/Noun', '공정/Noun', '이입/Noun', '협박/Noun', '추구/Noun', '추적/Noun', '개척/Noun', '성폭행/Noun', '부패/Noun', '무능력/Noun', '조장/Noun', '정복/Noun', '교묘/Noun', '구속/Noun', '야만/Noun', '편협/Noun', '해체/Noun', '응징/Noun', '주입/Noun', '달성/Noun', '겸손/Noun', '망각/Noun', '내포/Noun', '직시/Noun', '입증/Noun', '행세/Noun', '배출/Noun', '감수/Noun', '무수/Noun', '폭행/Noun', '몰살/Noun', '떳떳/Noun', '고단/Noun']\n",
      "\n",
      "Cluster 6\n",
      "['충무로/Noun', '빗/Noun', '장인/Noun', '서바이벌/Noun', '클럽/Noun', '천년/Noun', '대니/Noun', '발악/Noun', '박물관/Noun', '리그/Noun', '댐/Noun', '탐정/Noun', '괴담/Noun', '제국/Noun', '그날/Noun', '.,./Punctuation', '문자/Noun', '룰/Noun', '브라질/Noun', '악행/Noun', '레즈비언/Noun', '챔피언/Noun', '신자/Noun', '담당/Noun', '탐험/Noun', '자폐/Noun', '유괴/Noun', '대소/Noun', '해군/Noun', '소속/Noun', '징기스칸/Noun', '호주/Noun', '사탄/Noun', '퇴/Noun', '바위/Noun', '상위/Noun', '상사/Noun', '롤/Noun', '흡혈귀/Noun', '김두한/Noun', '현시/Noun', '보트/Noun', '적히다/Verb', '연예계/Noun', '강철중/Noun', '외계/Noun', '더구나/Noun', '반대편/Noun', '추격씬/Noun', '로또/Noun', '해골/Noun', '해당/Noun', '화성/Noun', '카페/Noun', '프랑스인/Noun', '뷰티/Noun', '사살/Noun', '붐/Noun', '맨몸/Noun', '마린/Noun', '매진/Noun', '낙원/Noun', '사단/Noun', '둘러싸다/Verb', '도구/Noun', '술집/Noun', '리플리/Noun', '김일성/Noun', '회전/Noun', '포로/Noun', '별명/Noun', '배신자/Noun', '전원/Noun', '사생활/Noun', '일침/Noun', '무사/Noun', '조던/Noun', '플래툰/Noun', '의원/Noun', '프레임/Noun', '설원/Noun', '국정원/Noun', '런던/Noun', '진주만/Noun', '판정/Noun', '라붐/Noun', '특수부대/Noun', 'FBI/Alpha', '카톨릭/Noun', '해병/Noun', '조센징/Noun', '힘쓰다/Verb', '연쇄살인범/Noun', '식민지/Noun', '어부/Noun', '무릎/Noun', '베트남전/Noun', '태극권/Noun', '자라나다/Verb', '선장/Noun', '북극/Noun', '발언/Noun', '환각/Noun', '합창/Noun', '쪼가리/Noun', '윗/Noun', '에헴/Exclamation', '핀란드/Noun', '시베리아/Noun', '여호와/Noun', '부작용/Noun', '유물/Noun', '채팅/Noun', '식탁/Noun', '정은/Noun', '계집/Noun', '지역감정/Noun', '악덕/Noun', '노처녀/Noun', '투명인간/Noun', '시집/Noun', '라이벌/Noun', '해군기지/Noun', '기껏/Noun', '하와이/Noun', '원시인/Noun', '감시/Noun', '영창/Noun', 'ET/Alpha', '필리핀/Noun', '연합/Noun', '여대생/Noun', '가운/Noun', '성범죄/Noun', '밀양/Noun', '초원/Noun', '성추행/Noun', '비밥/Noun', '사정봉/Noun', '그림판/Noun', '저음/Noun', '위험성/Noun', '아우성/Noun', '코미디언/Noun', '집앞/Noun', '명함/Noun', '중력/Noun', '후임/Noun', '개싸움/Noun', '동상/Noun', '시신/Noun', '공범/Noun', '장미여관/Noun', '핵폭탄/Noun', '노스/Noun', '달랑/Noun', '장관/Noun', '레닌/Noun', '정철/Noun', '김준/Noun', '입원/Noun', '강물/Noun', '착륙/Noun', '엮어/Noun', '매수/Noun', '소총/Noun', '중위/Noun', '실리/Noun', '서퍼/Noun', '기둥/Noun', 'CCTV/Alpha', '버티고/Noun', '비행선/Noun', '분자/Noun', '여고/Noun', '규정/Noun', '동물원/Noun', '탈레반/Noun', '사공/Noun', '맨손/Noun', '미혼/Noun', '파일럿/Noun', '심볼/Noun', '검찰/Noun', '일행/Noun', '손바닥/Noun', '도요타/Noun', '재팬/Noun', '감염/Noun', '복수심/Noun', '인턴/Noun', '육식/Noun', '패전/Noun', '원생/Noun', '사이버/Noun', '열풍/Noun', '인민군/Noun', '가상현실/Noun', '양복/Noun', '어리광/Noun', '세탁소/Noun', '산자/Noun', '해원/Noun', '신기전/Noun', '알리샤/Noun', '칭하/Noun', '강화/Noun', '다분히/Adverb', '애정결핍/Noun', '지훈/Noun', '진희경/Noun', '사물/Noun', '냉전시대/Noun', '영적/Noun', '각국/Noun', '사후/Noun', '뱃속/Noun', '다이쥬/Noun', '망토/Noun', '연습생/Noun', '반죽/Noun', '현세/Noun', '남쪽/Noun']\n",
      "\n",
      "Cluster 7\n",
      "['유사하다/Adjective']\n",
      "\n",
      "Cluster 8\n",
      "['할아버지/Noun', '억울하다/Adjective', '침/Noun', '다/Noun', '불쌍/Noun', '디지다/Verb', '아가/Noun', '만지다/Verb', '비명/Noun', '분투/Noun', '꽥꽥/Adverb', '울때/Noun']\n",
      "\n",
      "Cluster 9\n",
      "['겠다/Verb', '구만/Noun', '볼걸/Noun', '하나요/Noun', '라곤/Josa', '라지/Josa', '라기보단/Josa']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(0, 10):\n",
    "    print(\"\\nCluster %d\" % cluster)\n",
    "    words = []\n",
    "    for i in range(0, len(word_centroid_map.values())):\n",
    "        if(idx[i] == cluster):\n",
    "            words.append(model.wv.index2word[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2961"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 149995/149995 [00:53<00:00, 2811.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 49997/49997 [00:17<00:00, 2821.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_centroids = np.zeros((len(train_x), num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for doc, tag in tqdm(train_docs):\n",
    "    train_centroids[counter] = create_bag_of_centroids(doc, word_centroid_map)\n",
    "    counter += 1\n",
    "\n",
    "test_centroids = np.zeros((len(test_x), num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter=0\n",
    "for doc, tag in tqdm(test_docs):\n",
    "    test_centroids[counter] = create_bag_of_centroids(doc, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149995, 2961) (49997, 2961)\n"
     ]
    }
   ],
   "source": [
    "print(train_centroids.shape, test_centroids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deera\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# hyper Parameter\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "feature_count = train_centroids.shape[1]\n",
    "hidden_layers = feature_count // 2\n",
    "label_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"one_hot:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, feature_count])\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "print(Y)\n",
    "Y_onehot = tf.one_hot(Y, 1)\n",
    "print(Y_onehot)\n",
    "# Y_onehot = tf.reshape(Y_onehot, [-1, 1])\n",
    "# print(Y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\deera\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# models\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "h0 = tf.layers.dense(X, hidden_layers, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "h0 = tf.nn.dropout(h0, 0.95)\n",
    "h1 = tf.layers.dense(h0, label_count, activation=None)\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y_onehot, logits=h1)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "predicted = tf.nn.sigmoid(h1)\n",
    "correct_pred = tf.equal(tf.round(predicted), Y_onehot)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0\tLoss: 0.658\tAcc: 74.27%\tTest Acc: 77.03%\n",
      "Step:    10\tLoss: 0.374\tAcc: 84.29%\tTest Acc: 82.79%\n",
      "Step:    20\tLoss: 0.315\tAcc: 86.68%\tTest Acc: 83.77%\n",
      "Step:    30\tLoss: 0.272\tAcc: 89.00%\tTest Acc: 84.40%\n",
      "Step:    40\tLoss: 0.227\tAcc: 91.62%\tTest Acc: 84.78%\n",
      "Step:    50\tLoss: 0.182\tAcc: 94.13%\tTest Acc: 85.13%\n",
      "Step:    60\tLoss: 0.143\tAcc: 95.99%\tTest Acc: 85.28%\n",
      "Step:    70\tLoss: 0.112\tAcc: 97.15%\tTest Acc: 85.37%\n",
      "Step:    80\tLoss: 0.090\tAcc: 97.85%\tTest Acc: 85.43%\n",
      "Step:    90\tLoss: 0.073\tAcc: 98.27%\tTest Acc: 85.42%\n",
      "Step:   100\tLoss: 0.061\tAcc: 98.55%\tTest Acc: 85.46%\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "for step in range(training_epochs + 1):\n",
    "    sess.run(optimizer, feed_dict={X: train_centroids, Y: train_y})\n",
    "    loss, _, acc = sess.run([cost, optimizer, accuracy], feed_dict={\n",
    "                             X: train_centroids, Y: train_y})\n",
    "    if step % 10 == 0:\n",
    "        correct_prediction = tf.equal(tf.round(predicted), Y_onehot)\n",
    "        test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        test_accuracy_result = sess.run(test_accuracy, feed_dict={X:test_centroids, Y:test_y})\n",
    "        print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\\tTest Acc: {:.2%}\".format(\n",
    "            step, loss, acc, test_accuracy_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuray:  0.8546713\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.round(predicted), Y_onehot)\n",
    "test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('accuray: ', sess.run(test_accuracy, feed_dict={X:test_centroids, Y:test_y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# hyper Parameter\n",
    "learning_rate = 0.001\n",
    "training_epochs = 1000\n",
    "feature_count = len(train_x[0])\n",
    "hidden_layers = feature_count // 2\n",
    "label_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"one_hot:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, feature_count])\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "print(Y)\n",
    "Y_onehot = tf.one_hot(Y, 1)\n",
    "print(Y_onehot)\n",
    "# Y_onehot = tf.reshape(Y_onehot, [-1, 1])\n",
    "# print(Y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# models\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "h0 = tf.layers.dense(X, hidden_layers, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "h0 = tf.nn.dropout(h0, 0.95)\n",
    "h1 = tf.layers.dense(h0, label_count, activation=None)\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y_onehot, logits=h1)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "predicted = tf.nn.sigmoid(h1)\n",
    "correct_pred = tf.equal(tf.round(predicted), Y_onehot)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0\tLoss: 0.692\tAcc: 51.73%\tTest Acc: 54.78%\n",
      "Step:   100\tLoss: 0.574\tAcc: 68.51%\tTest Acc: 68.05%\n",
      "Step:   200\tLoss: 0.530\tAcc: 72.79%\tTest Acc: 69.98%\n",
      "Step:   300\tLoss: 0.503\tAcc: 74.96%\tTest Acc: 70.37%\n",
      "Step:   400\tLoss: 0.485\tAcc: 76.25%\tTest Acc: 70.04%\n",
      "Step:   500\tLoss: 0.471\tAcc: 77.13%\tTest Acc: 70.16%\n",
      "Step:   600\tLoss: 0.461\tAcc: 77.76%\tTest Acc: 70.06%\n",
      "Step:   700\tLoss: 0.453\tAcc: 78.27%\tTest Acc: 69.91%\n",
      "Step:   800\tLoss: 0.447\tAcc: 78.56%\tTest Acc: 69.67%\n",
      "Step:   900\tLoss: 0.441\tAcc: 78.99%\tTest Acc: 69.53%\n",
      "Step:  1000\tLoss: 0.435\tAcc: 79.32%\tTest Acc: 69.23%\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "for step in range(training_epochs + 1):\n",
    "    sess.run(optimizer, feed_dict={X: train_x, Y: train_y})\n",
    "    loss, _, acc = sess.run([cost, optimizer, accuracy], feed_dict={\n",
    "                             X: train_x, Y: train_y})\n",
    "    if step % 100 == 0:\n",
    "        correct_prediction = tf.equal(tf.round(predicted), Y_onehot)\n",
    "        test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        test_accuracy_result = sess.run(test_accuracy, feed_dict={X:test_x, Y:test_y})\n",
    "        print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\\tTest Acc: {:.2%}\".format(\n",
    "            step, loss, acc, test_accuracy_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuray:  0.69464165\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.round(predicted), Y_onehot)\n",
    "test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('accuray: ', sess.run(test_accuracy, feed_dict={X:test_x, Y:test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
